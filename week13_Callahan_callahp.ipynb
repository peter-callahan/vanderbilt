{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "Copy of HW.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peter-callahan/vanderbilt/blob/main/week13_Callahan_callahp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeIXFfAumNWV"
      },
      "source": [
        "#Convolutional Neural Network\n",
        "The goal of this assignment is to implement the convolutional neural network Pytorch to perform classification and test it out on the CIFAR-10 dataset. All the code will be implemented in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BD5dqNGm1b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53bf9407-5989-4f00-aca0-69db2fac53d2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3HggVCdm8iV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4739f46b-055c-4aee-ed94-791cf6d86d93"
      },
      "source": [
        "%ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mgdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soWnE8qOuQwg"
      },
      "source": [
        "First, let's install modules not already installed by Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75XndHcNuL0r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a223b1f3-c5fc-43d3-e694-f96af1318418"
      },
      "source": [
        "! pip install torch_utils"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch_utils\n",
            "  Downloading torch-utils-0.1.2.tar.gz (4.9 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torch_utils) (1.12.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torch_utils) (4.1.1)\n",
            "Building wheels for collected packages: torch-utils\n",
            "  Building wheel for torch-utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-utils: filename=torch_utils-0.1.2-py3-none-any.whl size=6202 sha256=b33180d883e2f5a9fb47dd4435ec661eec5bffbded315509458801554692a8ca\n",
            "  Stored in directory: /root/.cache/pip/wheels/5d/c0/1d/a539c1c2a4d41c5d7109899289cded24fe1320b6a6c7b02a4c\n",
            "Successfully built torch-utils\n",
            "Installing collected packages: torch-utils\n",
            "Successfully installed torch-utils-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mzuptv--6uXv"
      },
      "source": [
        "## Task 1: Design Your Convolutional Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTcfx8MFFxRz"
      },
      "source": [
        "In the begining, please import all the package you need. We provide some packages here, which might be helpful when you build your code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4wg5xqcxNEi"
      },
      "source": [
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "\n",
        "# import modules\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch import cuda\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader, sampler\n",
        "import torch.nn.functional as F\n",
        "from torch_utils import AverageMeter\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from numpy import inf\n",
        "import torchvision\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "from glob import glob\n",
        "from torchvision import transforms\n",
        "from torchvision import datasets\n",
        "from torchvision import models\n",
        "from torch import optim, cuda, Tensor\n",
        "import tqdm\n",
        "\n",
        "# Data science tools\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "\n",
        "# Image manipulations\n",
        "from PIL import Image\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "# Visualizations\n",
        "import matplotlib.pyplot as plt\n",
        "#plt.rcParams['font.size'] = 14\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPX2j9gCHFOW"
      },
      "source": [
        "Then, you need to define a class for your CNN. The network should be two conv layers, two pool layers, and three linear layers. You can follow the instruction to build your network.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tqHkSTcemYK"
      },
      "source": [
        "**I defined my model in two steps. The first step is to specify the parameters of my model, and the second step is to outline how they are applied to the inputs. I initialized the layers used in our model, which is Conv2d, Maxpool2d, and Linear layers. The forward method defines the feed-forward operation on the input data x. my conv1 layer is initialized with 3 input channels, 6 output channels, and a kernel size of 5. After that, I added a pooling layer, which downsamples my feature maps by summarizing features in patches of the feature map. Next, I flattened the last convolutional or pooling layer's output so it can be fed into a fully connected neural network to map the features extracted to their corresponding classes. In the forward method, I added ReLU activation to the layer's output.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDl90_EICBFp"
      },
      "source": [
        "# define model\n",
        "class bmodel(nn.Module):\n",
        "    def __init__(self):\n",
        "\n",
        "        # Define all the layers that you need in your network\n",
        "        # You can use nn.Conv2d() to define the 2d convolutional layer\n",
        "        # You can use nn.MaxPool2d() to define the 2d maxpooling layer\n",
        "        # You can use nn.Linear() to define the linear layer\n",
        "        # You can use F.relu() to define your ReLu layer\n",
        "        #####################\n",
        "        ### YOUR CODE HERE###\n",
        "        #####################\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3,6,3)\n",
        "        self.pool = nn.MaxPool2d(2,2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84,10)\n",
        "        ####################\n",
        "        ### YOUR CODE END###\n",
        "        ####################\n",
        "    def forward(self, x):\n",
        "        # You may use x.view() to reshape the tensor.\n",
        "        #####################\n",
        "        ### YOUR CODE HERE###\n",
        "        #####################\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "        ####################\n",
        "        ### YOUR CODE END###\n",
        "        ####################\n",
        "  \n",
        "model = bmodel()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clIyJmUFJg0H"
      },
      "source": [
        "Cuda is Compute Unified Device Architecture, which can achieve parallel computing. It will improve your learning speed in your parameter update by using GPU rather than CPU."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check whether there is a gpu for cuda\n",
        "train_on_gpu = cuda.is_available()\n",
        "print(f'Train on gpu: {train_on_gpu}')\n",
        "\n",
        "# Number of gpus\n",
        "if train_on_gpu:\n",
        "    gpu_count = cuda.device_count()\n",
        "    print(f'{gpu_count} gpus detected.')\n",
        "    if gpu_count > 1:\n",
        "        multi_gpu = True\n",
        "    else:\n",
        "        multi_gpu = False\n",
        "else:\n",
        "    multi_gpu = False\n",
        "print(train_on_gpu,multi_gpu)\n",
        "\n",
        "if train_on_gpu:\n",
        "    model = model.to('cuda')\n"
      ],
      "metadata": {
        "id": "cK_-FzDmiaNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HD4FczAuI6fK"
      },
      "source": [
        "First, we will use the CIFAR-10 dataset to train our model. In HW2 and HW3, we simply define a two-layer-network with linear layers. Therefore, we reshaped each image in one dimension when loading the data. In this assignment, we need to reshape our dataset within this shape [image number, rgb channels, height, weight] to match the convolutional network."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from data_utils import load_CIFAR10\n",
        "\n",
        "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000):\n",
        "    \"\"\"\n",
        "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
        "    it for the two-layer neural net classifier.   \n",
        "    \"\"\"\n",
        "    # Load the raw CIFAR-10 data\n",
        "    cifar10_dir = './datasets/'\n",
        "    \n",
        "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
        "        \n",
        "    # Subsample the data\n",
        "    mask = list(range(num_training, num_training + num_validation))\n",
        "    X_val = X_train[mask]\n",
        "    y_val = y_train[mask]\n",
        "    mask = list(range(num_training))\n",
        "    X_train = X_train[mask]\n",
        "    y_train = y_train[mask]\n",
        "    mask = list(range(num_test))\n",
        "    X_test = X_test[mask]\n",
        "    y_test = y_test[mask]\n",
        "\n",
        "    # Normalize the data: \n",
        "    X_train = X_train/X_train.max()\n",
        "    X_val = X_val/X_val.max()\n",
        "    X_test = X_test/X_test.max()\n",
        "\n",
        "\n",
        "    #Reshape data\n",
        "    # The shape should be [image number, rgb channels, height, weight]\n",
        "    # You can use np.moveaxis() to change the dimension order\n",
        "    \n",
        "    ######################\n",
        "    ### YOUR CODE HERE ###\n",
        "    ######################\n",
        "     \n",
        "    #####################\n",
        "    ### YOUR CODE END ###\n",
        "    #####################\n",
        "\n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
        "\n",
        "\n",
        "# Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n",
        "try:\n",
        "   del X_train, y_train\n",
        "   del X_test, y_test\n",
        "   print('Clear previously loaded data.')\n",
        "except:\n",
        "   pass\n",
        "\n",
        "# Invoke the above function to get our data.\n",
        "train_X, train_Y, validation_X, validation_Y, test_X, test_Y = get_CIFAR10_data()\n",
        "print('Train data shape: ', train_X.shape)\n",
        "print('Train labels shape: ', train_Y.shape)\n",
        "print('Validation data shape: ', validation_X.shape)\n",
        "print('Validation labels shape: ', validation_Y.shape)\n",
        "print('Test data shape: ', test_X.shape)\n",
        "print('Test labels shape: ', test_Y.shape)"
      ],
      "metadata": {
        "id": "mrbaxl1kijGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9kKSuJgKI-a"
      },
      "source": [
        "We can use the same code in HW3 for dataloader.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Datasets organization\n",
        "batch_size = 4\n",
        "\n",
        "# Transfer the data from numpy to tensor\n",
        "# You can use the same code in HW3\n",
        "data = {\n",
        "    'train':\n",
        "    TensorDataset(torch.from_numpy(train_X), torch.from_numpy(train_Y).float()),\n",
        "    'valid':\n",
        "    TensorDataset(torch.from_numpy(validation_X), torch.from_numpy(validation_Y).float())\n",
        "}\n",
        "# Dataloader iterators, make sure to shuffle\n",
        "# You can use the same code in HW3\n",
        "dataloaders = {\n",
        "    'train': DataLoader(data['train'], batch_size=batch_size, shuffle=True,num_workers=10),\n",
        "    'valid': DataLoader(data['valid'], batch_size=batch_size, shuffle=True,num_workers=10)\n",
        "}\n",
        "\n",
        "# Iterate through the dataloader once\n",
        "trainiter = iter(dataloaders['train'])\n",
        "validationiter = iter(dataloaders['valid'])"
      ],
      "metadata": {
        "id": "MSQoBoYNimna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hrq5MMe2Kco7"
      },
      "source": [
        "CIFAR-10 has 10 classes, which are shown below. We can print the images with labels to verify the dataset. Since we've reshaped our image data for training, they need to be reshaped for printing."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "# functions to show an image\n",
        "\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    # Reshape the image from [rgb_channel, weight, height] to [weight, height, rgb_channel]\n",
        "    # You can use np.transpose() to reorder the dimensions\n",
        "    ######################\n",
        "    ### YOUR CODE HERE ###\n",
        "    ######################\n",
        "    \n",
        "    #####################\n",
        "    ### YOUR CODE END ###\n",
        "    #####################\n",
        "    plt.imshow(npimg)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# get some random training images\n",
        "# you may use .next() to get the next iteration of training dataloader\n",
        "######################\n",
        "### YOUR CODE HERE ###\n",
        "######################\n",
        "\n",
        "#####################\n",
        "### YOUR CODE END ###\n",
        "#####################\n",
        "\n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "# print labels\n",
        "print(' '.join('%5s' % classes[labels[j].long()] for j in range(batch_size)))"
      ],
      "metadata": {
        "id": "KIbxANj4iqCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPQQ7iRaNgIp"
      },
      "source": [
        "Looks good! Now we may set up the loss function and theoptimizer tool."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTpFzjGxX8cP"
      },
      "source": [
        "# Set up your criterion and optimizer\n",
        "# You can use nn.CrossEntropyLoss() as your critenrion\n",
        "# You can use optim.SGD() as your optimizer\n",
        "\n",
        "#####################\n",
        "### YOUR CODE HERE###\n",
        "#####################\n",
        "\n",
        "####################\n",
        "### YOUR CODE END###\n",
        "####################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5ep_QqOOiNn"
      },
      "source": [
        "Now we can use the training process in HW3 to train our CNN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vw16i2tlOcbt"
      },
      "source": [
        "# You can use your train function in HW3\n",
        "def train(model,\n",
        "          criterion,\n",
        "          optimizer,\n",
        "          train_loader,\n",
        "          valid_loader,\n",
        "          save_file_name,\n",
        "          max_epochs_stop=3,\n",
        "          n_epochs=10,\n",
        "          print_every=1):\n",
        "    \"\"\"Train a PyTorch Model\n",
        "\n",
        "    Params\n",
        "    --------\n",
        "        model (PyTorch model): cnn to train\n",
        "        criterion (PyTorch loss): objective to minimize\n",
        "        optimizer (PyTorch optimizier): optimizer to compute gradients of model parameters\n",
        "        train_loader (PyTorch dataloader): training dataloader to iterate through\n",
        "        valid_loader (PyTorch dataloader): validation dataloader used for early stopping\n",
        "        save_file_name (str ending in '.pt'): file path to save the model state dict\n",
        "        max_epochs_stop (int): maximum number of epochs with no improvement in validation loss for early stopping\n",
        "        n_epochs (int): maximum number of training epochs\n",
        "        print_every (int): frequency of epochs to print training stats\n",
        "\n",
        "    Returns\n",
        "    --------\n",
        "        model (PyTorch model): trained cnn with best weights\n",
        "        history (DataFrame): history of train and validation loss and accuracy\n",
        "    \"\"\"\n",
        "\n",
        "    # Early stopping intialization\n",
        "    epochs_no_improve = 0\n",
        "    valid_loss_min = np.Inf\n",
        "\n",
        "    valid_max_acc = 0\n",
        "    history = []\n",
        "\n",
        "    # Number of epochs already trained (if using loaded in model weights)\n",
        "    try:\n",
        "        print(f'Model has been trained for: {model.epochs} epochs.\\n')\n",
        "    except:\n",
        "        model.epochs = 0\n",
        "        print(f'Starting Training from Scratch.\\n')\n",
        "\n",
        "    overall_start = timer()\n",
        "\n",
        "    # Main loop\n",
        "    for epoch in range(n_epochs):\n",
        "\n",
        "        # keep track of training and validation loss each epoch        \n",
        "        train_loss = 0.0\n",
        "        valid_loss = 0.0\n",
        "\n",
        "        train_acc = 0\n",
        "        valid_acc = 0       \n",
        "\n",
        "        # Set to training  \n",
        "        model.train()        \n",
        "        start = timer()\n",
        "\n",
        "        # Training loop\n",
        "        for ii, (data, target) in enumerate(train_loader):\n",
        "            \n",
        "            # Tensors to gpu            \n",
        "            if train_on_gpu:\n",
        "                model = model.cuda()\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "           \n",
        "            # Clear gradients\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "\n",
        "            model = model.float()\n",
        "            # Run the forward path, using model()\n",
        "            #####################\n",
        "            ### YOUR CODE HERE###\n",
        "            #####################            \n",
        "            \n",
        "            ####################\n",
        "            ### YOUR CODE END###\n",
        "            ####################            \n",
        "\n",
        "            # Compute loss function\n",
        "            #####################\n",
        "            ### YOUR CODE HERE###\n",
        "            #####################       \n",
        "            \n",
        "            ####################\n",
        "            ### YOUR CODE END###\n",
        "            ####################\n",
        "\n",
        "            # Run backward path and update the parameters\n",
        "            #####################\n",
        "            ### YOUR CODE HERE###\n",
        "            #####################     \n",
        "            \n",
        "            ####################\n",
        "            ### YOUR CODE END###\n",
        "            ####################         \n",
        "            \n",
        "            # Track train loss by multiplying average loss by number of examples in batch            \n",
        "            train_loss += loss.item() * data.size(0)\n",
        "            \n",
        "            # Calculate accuracy by finding max log probability            \n",
        "            _, pred = torch.max(output, dim=1)\n",
        "            correct_tensor = pred.eq(target.data.view_as(pred))\n",
        "           \n",
        "            # Need to convert correct tensor from int to float to average            \n",
        "            accuracy = torch.mean(correct_tensor.type(torch.FloatTensor))\n",
        "            \n",
        "            # Multiply average accuracy times the number of examples in batch            \n",
        "            train_acc += accuracy.item() * data.size(0)\n",
        "            \n",
        "            # Track training progress\n",
        "            print(\n",
        "                f'Epoch: {epoch}\\t{100 * (ii + 1) / len(train_loader):.2f}% complete. {timer() - start:.2f} seconds elapsed in epoch.',\n",
        "                end='\\r')\n",
        "\n",
        "        # After training loops ends, start validation\n",
        "        else:\n",
        "            model.epochs += 1\n",
        "\n",
        "            # Don't need to keep track of gradients\n",
        "            with torch.no_grad():\n",
        "\n",
        "                # Set to evaluation mode                \n",
        "                model.eval()               \n",
        "\n",
        "                # Validation loop\n",
        "                for data, target in valid_loader:\n",
        "                    # Tensors to gpu\n",
        "                    \n",
        "                    if train_on_gpu:\n",
        "                        model = model.cuda()\n",
        "                        data, target = data.cuda(), target.cuda()\n",
        "                    \n",
        "                    # Forward pass                    \n",
        "                    model = model.float()\n",
        "                    # Run the forward path, using model()\n",
        "                    #####################\n",
        "                    ### YOUR CODE HERE###\n",
        "                    #####################     \n",
        "                    \n",
        "                    ####################\n",
        "                    ### YOUR CODE END###\n",
        "                    ####################      \n",
        "                    \n",
        "     \n",
        "                    # Compute loss function\n",
        "                    #####################\n",
        "                    ### YOUR CODE HERE###\n",
        "                    #####################               \n",
        "                    \n",
        "                    ####################\n",
        "                    ### YOUR CODE END###\n",
        "                    ####################      \n",
        "                    \n",
        "                    # Multiply average loss times the number of examples in batch                    \n",
        "                    valid_loss += loss.item() * data.size(0)                    \n",
        "\n",
        "                    # Calculate validation accuracy                    \n",
        "                    _, pred = torch.max(output, dim=1)\n",
        "                    correct_tensor = pred.eq(target.data.view_as(pred))\n",
        "                    accuracy = torch.mean(\n",
        "                        correct_tensor.type(torch.FloatTensor))                    \n",
        "\n",
        "                    # Multiply average accuracy times the number of examples                   \n",
        "                    valid_acc += accuracy.item() * data.size(0)                    \n",
        "\n",
        "                # Calculate average losses                \n",
        "                train_loss = train_loss / len(train_loader.dataset)\n",
        "                valid_loss = valid_loss / len(valid_loader.dataset)                \n",
        "\n",
        "                # Calculate average accuracy                \n",
        "                train_acc = train_acc / len(train_loader.dataset)\n",
        "                valid_acc = valid_acc / len(valid_loader.dataset)                \n",
        "\n",
        "                history.append([train_loss, valid_loss, train_acc, valid_acc])\n",
        "\n",
        "                # Print training and validation results\n",
        "                if (epoch + 1) % print_every == 0:\n",
        "                    print(\n",
        "                        f'\\nEpoch: {epoch} \\tTraining Loss: {train_loss:.4f} \\tValidation Loss: {valid_loss:.4f}'\n",
        "                    )\n",
        "                    print(\n",
        "                        f'\\t\\tTraining Accuracy: {100 * train_acc:.2f}%\\t Validation Accuracy: {100 * valid_acc:.2f}%'\n",
        "                    )\n",
        "\n",
        "                # Save the model if validation loss decreases\n",
        "                if valid_loss < valid_loss_min:\n",
        "                    # Save model\n",
        "                    torch.save(model.state_dict(), save_file_name)\n",
        "                    # Track improvement\n",
        "                    epochs_no_improve = 0\n",
        "                    valid_loss_min = valid_loss\n",
        "                    valid_best_acc = valid_acc\n",
        "                    best_epoch = epoch\n",
        "\n",
        "                # Otherwise increment count of epochs with no improvement\n",
        "                else:\n",
        "                    epochs_no_improve += 1\n",
        "                    # Trigger early stopping\n",
        "                    if epochs_no_improve >= max_epochs_stop:\n",
        "                        print(\n",
        "                            f'\\nEarly Stopping! Total epochs: {epoch}. Best epoch: {best_epoch} with loss: {valid_loss_min:.2f} and acc: {100 * valid_acc:.2f}%'\n",
        "                        )\n",
        "                        total_time = timer() - overall_start\n",
        "                        print(\n",
        "                            f'{total_time:.2f} total seconds elapsed. {total_time / (epoch+1):.2f} seconds per epoch.'\n",
        "                        )\n",
        "\n",
        "                        # Load the best state dict                        \n",
        "                        model.load_state_dict(torch.load(save_file_name))\n",
        "                        \n",
        "                        # Attach the optimizer\n",
        "                        model.optimizer = optimizer\n",
        "\n",
        "                        # Format history\n",
        "                        history = pd.DataFrame(\n",
        "                            history,\n",
        "                            columns=[\n",
        "                                'train_loss', 'valid_loss', 'train_acc',\n",
        "                                'valid_acc'\n",
        "                            ])\n",
        "                        return model, history\n",
        "\n",
        "    # Attach the optimizer\n",
        "    model.optimizer = optimizer\n",
        "    # Record overall time and print out stats\n",
        "    total_time = timer() - overall_start\n",
        "    print(\n",
        "        f'\\nBest epoch: {best_epoch} with loss: {valid_loss_min:.2f} and acc: {100 * valid_best_acc:.2f}%'\n",
        "    )\n",
        "    print(\n",
        "        f'{total_time:.2f} total seconds elapsed. {total_time / (epoch+1):.2f} seconds per epoch.'\n",
        "    )\n",
        "    # Format history\n",
        "    history = pd.DataFrame(\n",
        "        history,\n",
        "        columns=['train_loss', 'valid_loss', 'train_acc', 'valid_acc'])\n",
        "    return model, history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdRJcmzXReJT"
      },
      "source": [
        "Once we set up everything, we can start to train our CNN."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from timeit import default_timer as timer\n",
        "save_file_name = f'CNN_model_best_model.pt'\n",
        "train_on_gpu = cuda.is_available()\n",
        "\n",
        "model, history = train(model,\n",
        "    criterion,\n",
        "    optimizer,\n",
        "    dataloaders['train'], \n",
        "    dataloaders['valid'],\n",
        "    save_file_name=save_file_name,\n",
        "    max_epochs_stop=3,\n",
        "    n_epochs=500,\n",
        "    print_every=1\n",
        "    )"
      ],
      "metadata": {
        "id": "ad7SNcmdi0d6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5oRuk5hSEYx"
      },
      "source": [
        "At this time, we use CNN, which can get better features from images than two-layer-network. The results should be better than HW3. Now, we can check the losses and accuracy during the training."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "for c in ['train_loss', 'valid_loss']:\n",
        "    plt.plot(\n",
        "        history[c], label=c)\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Average Negative Log Likelihood')\n",
        "plt.title('Training and Validation Losses')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "u5v1dItmi59N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "for c in ['train_acc', 'valid_acc']:\n",
        "    plt.plot(\n",
        "        100 * history[c], label=c)\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Average Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nPHPhR4ii72n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfbqTBqQMXLt"
      },
      "source": [
        "A big progress! You may wondering whether your network can predict correctly. You may use your model to get the prediction with validationset."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataiter = iter(validationiter)\n",
        "# get some random training images\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# Get the prediction of images by using your model.\n",
        "outputs = model(images.cuda().float())\n",
        "_, predicted = torch.max(outputs, 1)\n",
        "\n",
        "# print images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j].long()] for j in range(batch_size)))\n",
        "print('Prediction: ', ' '.join('%5s' % classes[predicted[j].long()] for j in range(batch_size)))"
      ],
      "metadata": {
        "id": "gQ6YW9GEi-5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhsY7oawmNWu"
      },
      "source": [
        "##Task 2: Improve your performance\n",
        "Here, we may (1) add more layers to make the network deeper, or (2) replace your bmodel() with networks provided by PyTorch. https://pytorch.org/vision/0.8/models.html\n",
        "You just need to do one of these two options.\n",
        "\n",
        "You can reuse the code you have from Task 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUvHA1B8g2Qx"
      },
      "source": [
        "**In order to improve my performance, I replaced my bmodel() with a network provided by PyTorch. Among many networks, I chosed Resnet18. This network is from the paper \"Deep Residual Learning for Image Recognition\". This model reformulates deep learning's layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. When I used bmodel(), the validation accuracy is 60.40%, but now, it got higher to 75.80% by using Resnet18 model.**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models as models\n",
        "resnet18 = models.resnet18()\n",
        "\n",
        "from data_utils import load_CIFAR10\n",
        "\n",
        "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000):\n",
        "    \"\"\"\n",
        "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
        "    it for the two-layer neural net classifier.   \n",
        "    \"\"\"\n",
        "    # Load the raw CIFAR-10 data\n",
        "    cifar10_dir = './datasets/'\n",
        "    \n",
        "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
        "        \n",
        "    # Subsample the data\n",
        "    mask = list(range(num_training, num_training + num_validation))\n",
        "    X_val = X_train[mask]\n",
        "    y_val = y_train[mask]\n",
        "    mask = list(range(num_training))\n",
        "    X_train = X_train[mask]\n",
        "    y_train = y_train[mask]\n",
        "    mask = list(range(num_test))\n",
        "    X_test = X_test[mask]\n",
        "    y_test = y_test[mask]\n",
        "\n",
        "    # Normalize the data: \n",
        "    X_train = X_train/X_train.max()\n",
        "    X_val = X_val/X_val.max()\n",
        "    X_test = X_test/X_test.max()\n",
        "\n",
        "\n",
        "    #Reshape data\n",
        "    # The shape should be [image number, rgb channels, height, weight]\n",
        "    # You can use np.moveaxis() to change the dimension order\n",
        "    \n",
        "    ######################\n",
        "    ### YOUR CODE HERE ###\n",
        "    ######################\n",
        "     \n",
        "    #####################\n",
        "    ### YOUR CODE END ###\n",
        "    #####################\n",
        "\n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
        "\n",
        "\n",
        "# Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n",
        "try:\n",
        "   del X_train, y_train\n",
        "   del X_test, y_test\n",
        "   print('Clear previously loaded data.')\n",
        "except:\n",
        "   pass\n",
        "\n",
        "# Invoke the above function to get our data.\n",
        "train_X, train_Y, validation_X, validation_Y, test_X, test_Y = get_CIFAR10_data()\n",
        "print('Train data shape: ', train_X.shape)\n",
        "print('Train labels shape: ', train_Y.shape)\n",
        "print('Validation data shape: ', validation_X.shape)\n",
        "print('Validation labels shape: ', validation_Y.shape)\n",
        "print('Test data shape: ', test_X.shape)\n",
        "print('Test labels shape: ', test_Y.shape)"
      ],
      "metadata": {
        "id": "QIQ5BK2wjD1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Datasets organization\n",
        "batch_size = 4\n",
        "\n",
        "# Transfer the data from numpy to tensor\n",
        "# You can use the same code in HW3\n",
        "data = {\n",
        "    'train':\n",
        "    TensorDataset(torch.from_numpy(train_X), torch.from_numpy(train_Y).float()),\n",
        "    'valid':\n",
        "    TensorDataset(torch.from_numpy(validation_X), torch.from_numpy(validation_Y).float())\n",
        "}\n",
        "# Dataloader iterators, make sure to shuffle\n",
        "# You can use the same code in HW3\n",
        "dataloaders = {\n",
        "    'train': DataLoader(data['train'], batch_size=batch_size, shuffle=True,num_workers=10),\n",
        "    'valid': DataLoader(data['valid'], batch_size=batch_size, shuffle=True,num_workers=10)\n",
        "}\n",
        "\n",
        "# Iterate through the dataloader once\n",
        "trainiter = iter(dataloaders['train'])\n",
        "validationiter = iter(dataloaders['valid'])"
      ],
      "metadata": {
        "id": "NXiNidTnjGr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVYDdEsVx0ST"
      },
      "source": [
        "# Set up your criterion and optimizer\n",
        "# You can use nn.CrossEntropyLoss() as your critenrion\n",
        "# You can use optim.SGD() as your optimizer\n",
        "\n",
        "#####################\n",
        "### YOUR CODE HERE###\n",
        "#####################\n",
        "\n",
        "####################\n",
        "### YOUR CODE END###\n",
        "####################\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from timeit import default_timer as timer\n",
        "save_file_name = f'CNN_model_best_model.pt'\n",
        "train_on_gpu = cuda.is_available()\n",
        "\n",
        "model, history = train(resnet18,\n",
        "    criterion,\n",
        "    optimizer,\n",
        "    dataloaders['train'], \n",
        "    dataloaders['valid'],\n",
        "    save_file_name=save_file_name,\n",
        "    max_epochs_stop=3,\n",
        "    n_epochs=500,\n",
        "    print_every=1\n",
        "    )"
      ],
      "metadata": {
        "id": "jlXR2tIJjJXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSXTbjzuyuke"
      },
      "source": [
        "Please plot the figures and show the prediction of your network."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "for c in ['train_loss', 'valid_loss']:\n",
        "    plt.plot(\n",
        "        history[c], label=c)\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Average Negative Log Likelihood')\n",
        "plt.title('Training and Validation Losses')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kMplHYa8jLib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "for c in ['train_acc', 'valid_acc']:\n",
        "    plt.plot(\n",
        "        100 * history[c], label=c)\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Average Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "S3pru2dVjOgH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}